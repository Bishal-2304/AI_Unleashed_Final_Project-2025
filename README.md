AI-Based Multi-Mode Human Computer Interaction System

ğŸ§© Overview
This project is an AI-driven Human-Computer Interaction (HCI) system that allows users to interact with their computer using Eye Movement, Hand Gestures, and Voice Commands.  
It provides three distinct control modes:
1. Eye Tracking Mode â€” Move and control the Windows cursor using your eye gaze.  
2. Hand Tracking Mode â€” Control cursor movement with hand gestures and perform actions like click or drag.  
3. Voice Control Calculator Mode â€” Perform arithmetic calculations through voice commands such as â€œseven plus threeâ€.
This project aims to provide touchless control for accessibility, innovation, and smarter human-computer interfaces.

---

âœ¨ Features
ğŸ‘ Eye Tracking Mode - Tracks the userâ€™s eye gaze using webcam feed and moves the cursor accordingly. Supports blink detection for click actions. 
âœ‹ Hand Tracking Mode - Detects hand movements and gestures using MediaPipe and OpenCV. Enables pointer movement, click (pinch), and drag operations. 
ğŸ™ Voice Control Calculator Mode - Accepts voice input through the browser microphone, recognizes spoken arithmetic expressions, and performs real-time calculations. 
ğŸ”’ Secure Authentication - JWT-based user authentication with protected routes. 
ğŸ’» Web-Based Interface - Clean, modern React UI for switching between different modes seamlessly. 
ğŸ“– Manual Pages - Step-by-step guides for each mode with troubleshooting tips and example commands. 

---

 ğŸ›  Technologies Used
 Frontend - React.js, React Router, HTML5, CSS3 
 Backend - Flask (Python) 
 AI / CV Libraries - OpenCV, MediaPipe, dlib 
 Voice Recognition - Web Speech API
 Authentication - JWT (JSON Web Token), Flask-CORS 
 Utilities - dotenv, Numpy 
 Design Tools - Tailwind / Custom CSS, Modern minimalist UI 

---

âš™ Installation
ğŸ”§ Prerequisites

Before running the project, ensure you have:
- Python 3.9
- Node.js 18 and npm
- Webcam and Microphone access enabled
- Modern browser (Google Chrome / Microsoft Edge recommended)

---

ğŸ§± Setup Instructions
 1. Clone the Repository
```bash
git clone https://github.com/Bishal-2304/AI_Unleashed_Final_Project-2025.git
cd AI_Unleashed_Final_Project-2025
2. Setup the Backend (Flask)
cd server
pip install -r requirements.txt
python app.py
3. Setup the Frontend (React)
cd client
npm install
npm run dev
4. Environment Variables
Create a .env file in the server directory and include:
SECRET_KEY=your_secret_key

ğŸ–± Usage
Launch both frontend and backend servers.
Open the app in your browser and log in.
Navigate to the Mode Selection page.
Choose your preferred mode:
ğŸ‘ Eye Tracking
âœ‹ Hand Tracking
ğŸ™ Voice Control Calculator
Follow the on-screen manual for each mode.
ğŸ– Gesture Definitions (Hand Tracking Mode)
Gesture	Action
âœ‹ Open Hand	Move the mouse cursor
ğŸ¤ Pinch (Thumb + Index)	Left Click
âœŠ Fist	Hold / Drag
âœŒ Two Fingers Up	Right Click
ğŸ–– Five Fingers	Exit or Stop Tracking
All gestures are tracked in real-time using MediaPipe Hands and mapped to system cursor control using PyAutoGUI.

ğŸ“‚ Folder Structure
ğŸ“¦ project-root
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env
â”‚   â””â”€â”€ ... (API endpoints for Eye and Hand tracking)
â”‚
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ ModeSelection.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ VoiceCalculator.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ VoiceManual.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ EyeManual.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ HandManual.jsx
â”‚   â”‚   â”‚   â””â”€â”€ CameraFeed.jsx
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”‚       â””â”€â”€ authFetch.js
â”‚   â”œâ”€â”€ styles/
â”‚   â”‚   â”œâ”€â”€ App.css
â”‚   â”‚   â”œâ”€â”€ index.css
â”‚   â”‚   â””â”€â”€ Voice-bg.jpg
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md

ğŸ¤ Contributing
Contributions are welcome!
If youâ€™d like to enhance features, improve accuracy, or extend functionality:
Fork the repositor
Create a new branch
git checkout -b feature/your-feature-name
Commit your changes
git commit -m "Add: new feature"
Push to your fork
git push origin feature/your-feature-name
Submit a Pull Request ğŸš€

ğŸ™ Acknowledgments
Special thanks to:
MediaPipe and OpenCV teams for real-time hand & eye tracking models.
Flask and React communities for powerful and flexible frameworks.
Google Web Speech API for enabling seamless voice recognition.
Mentors, peers, and contributors who provided support and feedback throughout development.

ğŸ§  Future Enhancements
ğŸ“¡ Multi-gesture recognition for scroll and zoom actions
ğŸ’¬ Voice feedback (text-to-speech results)
ğŸ§¾ Multi-step arithmetic parsing (â€œfive plus six times twoâ€)
âš¡ Native desktop version using Electron
ğŸ¤– Integration with accessibility tools for differently-abled users
